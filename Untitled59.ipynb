{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfBxx/0H/YhIBoUm60TCcB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sepidehasadi87/Bayesianspatial-zero-inflated-discrete-weibull/blob/master/Untitled59.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "k_19W_l_BMpR",
        "outputId": "ddc48b6a-7cad-4423-814f-275a263146b6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-3063275473.py, line 997)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3063275473.py\"\u001b[0;36m, line \u001b[0;32m997\u001b[0m\n\u001b[0;31m    simulator.run_full_pipeline()Result\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy scipy pandas matplotlib scikit-learn networkx\n",
        "!pip install torch torchvision torchaudio  # (برای PyTorch - بسته به سیستم، ممکن است نیاز به تنظیم CUDA داشته باشید؛ برای CPU: pip install torch)\n",
        "!pip install torch_geometric  # برای GCN در PyTorch Geometric\n",
        "!pip install numpy pandas matplotlib seaborn tqdm lifelines\n",
        "!pip install torch torchvision torchaudio torch-geometric\n",
        "!pip install scikit-learn scipy\n",
        "!pip install networkx\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from sklearn.metrics import roc_auc_score, mean_absolute_error, mean_squared_error\n",
        "import pickle\n",
        "from scipy import stats\n",
        "import networkx as nx\n",
        "\n",
        "# Survival libraries (optional)\n",
        "try:\n",
        "    from lifelines.utils import concordance_index as lifelines_cindex\n",
        "    from lifelines import KaplanMeierFitter\n",
        "    from lifelines.metrics import integrated_brier_score\n",
        "    HAS_LIFELINES = True\n",
        "except ImportError:\n",
        "    HAS_LIFELINES = False\n",
        "\n",
        "# PyTorch Geometric (optional)\n",
        "try:\n",
        "    from torch_geometric.nn import GCNConv\n",
        "    HAS_PYG = True\n",
        "except ImportError:\n",
        "    HAS_PYG = False\n",
        "\n",
        "# GeoPandas and NetworkX for maps (optional)\n",
        "try:\n",
        "    import geopandas as gpd\n",
        "    from shapely.geometry import Polygon\n",
        "    HAS_GEOPANDAS = True\n",
        "except ImportError:\n",
        "    HAS_GEOPANDAS = False\n",
        "\n",
        "sns.set(style=\"whitegrid\", font_scale=1.2, palette=\"muted\")\n",
        "np.random.seed(42)\n",
        "\n",
        "# Helper functions and Model classes (اضافه شده)\n",
        "\n",
        "def manual_c_index(y_true, event_observed, risk_score):\n",
        "    \"\"\"Manual calculation of concordance index for survival data.\"\"\"\n",
        "    # Ensure inputs are numpy arrays\n",
        "    y_true = np.asarray(y_true)\n",
        "    event_observed = np.asarray(event_observed)\n",
        "    risk_score = np.asarray(risk_score)\n",
        "\n",
        "    # Filter out censored observations where the event time is less than or equal to others\n",
        "    # We only consider pairs where the individual with the earlier time had an event\n",
        "    comparable_pairs = 0\n",
        "    concordant_pairs = 0\n",
        "\n",
        "    for i in range(len(y_true)):\n",
        "        for j in range(len(y_true)):\n",
        "            # Skip if same individual\n",
        "            if i == j:\n",
        "                continue\n",
        "\n",
        "            # Check if pair (i, j) is comparable\n",
        "            # i is comparable to j if y_true[i] < y_true[j] and event_observed[i] == 1\n",
        "            # or if y_true[i] == y_true[j] and event_observed[i] == 1 and event_observed[j] == 1\n",
        "            if (y_true[i] < y_true[j] and event_observed[i] == 1) or \\\n",
        "               (y_true[i] == y_true[j] and event_observed[i] == 1 and event_observed[j] == 1):\n",
        "\n",
        "                comparable_pairs += 1\n",
        "\n",
        "                # Check if concordant\n",
        "                # For survival data, lower risk score is better (higher survival probability)\n",
        "                # So concordant if risk_score[i] < risk_score[j]\n",
        "                if risk_score[i] < risk_score[j]:\n",
        "                    concordant_pairs += 1\n",
        "                elif risk_score[i] == risk_score[j]:\n",
        "                    # Handle ties in risk scores - count as 0.5\n",
        "                    concordant_pairs += 0.5\n",
        "\n",
        "    if comparable_pairs == 0:\n",
        "        return 0.5  # By convention, C-index is 0.5 if no comparable pairs\n",
        "\n",
        "    return concordant_pairs / comparable_pairs\n",
        "\n",
        "def simple_ibs_discrete(y_true, event_observed, q_pred, beta_pred, times):\n",
        "    \"\"\"\n",
        "    Simple implementation of Integrated Brier Score for discrete time.\n",
        "    Assumes y_true is observed time, event_observed is delta (1 if event, 0 if censored).\n",
        "    q_pred and beta_pred are parameters from the model.\n",
        "    times are the time points for evaluation.\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    event_observed = np.asarray(event_observed)\n",
        "    q_pred = np.asarray(q_pred)\n",
        "    beta_pred = np.asarray(beta_pred)\n",
        "    times = np.asarray(times) # Ensure times is a numpy array\n",
        "\n",
        "    n = len(y_true)\n",
        "    total_brier_score = 0\n",
        "    num_times = len(times)\n",
        "\n",
        "    for t in times:\n",
        "        # Kaplan-Meier estimate for censoring\n",
        "        km_censoring = KaplanMeierFitter() if HAS_LIFELINES else None\n",
        "        if HAS_LIFELINES:\n",
        "            km_censoring.fit(y_true, event_observed=1 - event_observed) # Fit on censoring events\n",
        "            prob_censored_after_t = km_censoring.predict(t) if t in km_censoring.survival_function_.index else (km_censoring.survival_function_.iloc[-1] if t > km_censoring.survival_function_.index[-1] else 1.0)\n",
        "        else:\n",
        "             # Simple censoring probability estimate (less accurate)\n",
        "             prob_censored_after_t = np.mean(y_true > t)\n",
        "\n",
        "        if prob_censored_after_t == 0:\n",
        "             # Avoid division by zero, skip this time point if no one is at risk\n",
        "             continue\n",
        "\n",
        "        # Calculate Brier Score at time t\n",
        "        brier_score_t = 0\n",
        "        num_at_risk = 0\n",
        "\n",
        "        for i in range(n):\n",
        "            # Only consider individuals who are at risk at time t (observed time >= t)\n",
        "            if y_true[i] >= t:\n",
        "                num_at_risk += 1\n",
        "                # Predicted survival probability at time t\n",
        "                S_hat_t = q_pred[i] ** (t ** beta_pred[i])\n",
        "\n",
        "                # Brier score contribution for individual i at time t\n",
        "                if y_true[i] > t:\n",
        "                    # Individual survived past time t\n",
        "                    brier_score_i = (0 - S_hat_t)**2\n",
        "                elif y_true[i] <= t and event_observed[i] == 1:\n",
        "                    # Individual had an event at or before time t\n",
        "                    brier_score_i = (1 - S_hat_t)**2\n",
        "                else:\n",
        "                    # Individual is censored before or at time t, not included in the sum\n",
        "                    continue\n",
        "\n",
        "                # Inverse Probability of Censoring Weighting (IPCW)\n",
        "                # Weight by the probability of not being censored before or at time t\n",
        "                weight_i = 1.0 / prob_censored_after_t # Simplified: using overall KM for censoring\n",
        "\n",
        "                brier_score_t += weight_i * brier_score_i\n",
        "\n",
        "        if num_at_risk > 0:\n",
        "            brier_score_t /= num_at_risk\n",
        "            total_brier_score += brier_score_t\n",
        "\n",
        "    if num_times == 0:\n",
        "        return 0.0, [] # Return 0 IBS if no times are provided\n",
        "    # Average over the time points\n",
        "    integrated_brier_score_val = total_brier_score / num_times\n",
        "\n",
        "    # Note: This is a simplified IBS. A proper implementation requires\n",
        "    # a more robust IPCW calculation and handling of continuous time.\n",
        "    return integrated_brier_score_val, [] # Returning empty list for times for simplicity\n",
        "\n",
        "\n",
        "def surv_nll_loss(q, beta, y, delta):\n",
        "    \"\"\"\n",
        "    Negative Log-Likelihood loss for standard discrete survival model\n",
        "    with Weibull-like survival function S(t) = q^t^beta.\n",
        "    y: observed time (discrete)\n",
        "    delta: event indicator (1 if event, 0 if censored)\n",
        "    \"\"\"\n",
        "    y = y.long() # Ensure y is long for indexing\n",
        "    q = torch.clamp(q, 1e-6, 1 - 1e-6) # Clamp q to avoid numerical issues\n",
        "    beta = torch.relu(beta) + 1e-6 # Ensure beta is positive\n",
        "\n",
        "    # Survival function S(t) = q^(t^beta)\n",
        "    # S(t-1) = q^((t-1)^beta)\n",
        "    # P(T=t) = S(t-1) - S(t) = q^((t-1)^beta) - q^(t^beta)\n",
        "\n",
        "    # Handle y=0 case separately if needed, but in this model T >= 1\n",
        "    # For y=1: P(T=1) = S(0) - S(1) = q^(0^beta) - q^(1^beta) = 1 - q^1\n",
        "    # For y>1: P(T=y) = S(y-1) - S(y) = q^((y-1)^beta) - q^(y^beta)\n",
        "\n",
        "    # Probability of event at time y: P(T=y) if delta=1\n",
        "    # Probability of surviving beyond time y: S(y) if delta=0\n",
        "\n",
        "    # Calculate S(y) and S(y-1)\n",
        "    S_y = q ** (y.float() ** beta)\n",
        "    S_y_minus_1 = torch.ones_like(y).float() # S(0) = 1\n",
        "    mask_y_gt_1 = (y > 1)\n",
        "    S_y_minus_1[mask_y_gt_1] = q[mask_y_gt_1] ** ((y[mask_y_gt_1].float() - 1) ** beta[mask_y_gt_1])\n",
        "\n",
        "    # Probability density at time y: P(T=y)\n",
        "    p_y = S_y_minus_1 - S_y\n",
        "    p_y = torch.clamp(p_y, 1e-9, 1 - 1e-9) # Clamp probability\n",
        "\n",
        "    # Loss for observed events (delta=1): -log(P(T=y))\n",
        "    event_loss = -torch.log(p_y[delta == 1])\n",
        "\n",
        "    # Loss for censored observations (delta=0): -log(S(y))\n",
        "    censoring_loss = -torch.log(S_y[delta == 0])\n",
        "\n",
        "    # Total NLL\n",
        "    total_loss = torch.sum(event_loss) + torch.sum(censoring_loss)\n",
        "\n",
        "    return total_loss / len(y) # Return mean NLL\n",
        "\n",
        "\n",
        "def zis_nll_loss(pi, q, beta, y, delta):\n",
        "    \"\"\"\n",
        "    Negative Log-Likelihood loss for Zero-Inflated Survival model.\n",
        "    pi: probability of being in the zero-inflation component\n",
        "    q: base survival parameter for the non-zero component\n",
        "    beta: shape parameter for the non-zero component survival\n",
        "    y: observed time (discrete)\n",
        "    delta: event indicator (1 if event, 0 if censored)\n",
        "    \"\"\"\n",
        "    y = y.long() # Ensure y is long for indexing\n",
        "    pi = torch.clamp(pi, 1e-6, 1 - 1e-6) # Clamp pi\n",
        "    q = torch.clamp(q, 1e-6, 1 - 1e-6) # Clamp q\n",
        "    beta = torch.relu(beta) + 1e-6 # Ensure beta is positive\n",
        "\n",
        "    # S_nz(t) = q^(t^beta)  (Survival function for non-zero component)\n",
        "    S_nz_y = q ** (y.float() ** beta)\n",
        "    S_nz_y_minus_1 = torch.ones_like(y).float() # S_nz(0) = 1\n",
        "    mask_y_gt_1 = (y > 1)\n",
        "    S_nz_y_minus_1[mask_y_gt_1] = q[mask_y_gt_1] ** ((y[mask_y_gt_1].float() - 1) ** beta[mask_y_gt_1])\n",
        "\n",
        "    # Probability density for non-zero component: p_nz(y) = S_nz(y-1) - S_nz(y)\n",
        "    p_nz_y = S_nz_y_minus_1 - S_nz_y\n",
        "    p_nz_y = torch.clamp(p_nz_y, 1e-9, 1 - 1e-9) # Clamp probability\n",
        "\n",
        "    # Overall probability of event at time y:\n",
        "    # If y_i = 0 (implies delta_i = 1 in this simulation setup)\n",
        "    # L_i = P(Y_i=0, delta_i=1) = pi_i\n",
        "\n",
        "    # If y_i > 0 and delta_i = 1 (event at time y_i):\n",
        "    # L_i = P(Y_i=y_i, delta_i=1) = P(T_i=y_i) = (1-pi_i) * P(T_nz_i=y_i)\n",
        "    # P(T_nz_i=y_i) = S_nz_i(y_i-1) - S_nz_i(y_i)\n",
        "    # L_i = (1-pi_i) * (S_nz_i(y_i-1) - S_nz_i(y_i))\n",
        "\n",
        "    # If y_i > 0 and delta_i = 0 (censored at time y_i):\n",
        "    # L_i = P(Y_i > y_i, delta_i=0) = P(T_i > y_i) = (1-pi_i) * P(T_nz_i > y_i)\n",
        "    # P(T_nz_i > y_i) = S_nz_i(y_i)\n",
        "    # L_i = (1-pi_i) * S_nz_i(y_i)\n",
        "\n",
        "    # Combine into negative log-likelihood:\n",
        "    loss = 0.0\n",
        "    epsilon = 1e-9 # for numerical stability\n",
        "\n",
        "    # Case 1: y_i = 0 (implies delta_i = 1 in this simulation setup)\n",
        "    mask_y_is_zero = (y == 0)\n",
        "    if torch.any(mask_y_is_zero):\n",
        "        loss += -torch.sum(torch.log(pi[mask_y_is_zero] + epsilon))\n",
        "\n",
        "    # Case 2: y_i > 0 and delta_i = 1 (event at time y_i > 0)\n",
        "    mask_event_nonzero = (y > 0) & (delta == 1)\n",
        "    if torch.any(mask_event_nonzero):\n",
        "        prob_event_at_y = (1 - pi[mask_event_nonzero]) * (S_nz_y_minus_1[mask_event_nonzero] - S_nz_y[mask_event_nonzero])\n",
        "        loss += -torch.sum(torch.log(prob_event_at_y + epsilon))\n",
        "\n",
        "    # Case 3: y_i > 0 and delta_i = 0 (censored at time y_i > 0)\n",
        "    mask_censored_nonzero = (y > 0) & (delta == 0)\n",
        "    if torch.any(mask_censored_nonzero):\n",
        "        prob_survive_past_y = (1 - pi[mask_censored_nonzero]) * S_nz_y[mask_censored_nonzero]\n",
        "        loss += -torch.sum(torch.log(prob_survive_past_y + epsilon))\n",
        "\n",
        "    return loss / len(y) # Return mean NLL\n",
        "\n",
        "\n",
        "class DeepSurvLike(nn.Module):\n",
        "    \"\"\"\n",
        "    A basic Feedforward Neural Network for survival analysis,\n",
        "    predicting parameters for a Weibull-like discrete survival model.\n",
        "    Similar structure to DeepSurv but adapted for discrete time.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dims=[64, 32]):\n",
        "        super(DeepSurvLike, self).__init__()\n",
        "        layers = []\n",
        "        current_dim = input_dim\n",
        "        for h_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(current_dim, h_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(0.2)) # Added dropout\n",
        "            current_dim = h_dim\n",
        "\n",
        "        self.feature_extractor = nn.Sequential(*layers)\n",
        "\n",
        "        # Output layer for q (base survival rate) and beta (shape parameter)\n",
        "        # q should be between 0 and 1. Use sigmoid or clamp after linear.\n",
        "        # beta should be positive. Use softplus or relu + epsilon.\n",
        "        self.q_layer = nn.Linear(current_dim, 1)\n",
        "        self.beta_layer = nn.Linear(current_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.feature_extractor(x)\n",
        "        q_logit = self.q_layer(features)\n",
        "        beta_raw = self.beta_layer(features)\n",
        "\n",
        "        q = torch.sigmoid(q_logit).squeeze(-1) # Sigmoid to ensure q is between 0 and 1\n",
        "        beta = F.softplus(beta_raw).squeeze(-1) + 1e-6 # Softplus to ensure beta is positive\n",
        "\n",
        "        return q, beta\n",
        "\n",
        "\n",
        "class ZIDeepSurv(nn.Module):\n",
        "    \"\"\"\n",
        "    Zero-Inflated DeepSurv-like model without spatial components.\n",
        "    Predicts pi (zero-inflation prob), q, and beta based only on covariates X.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dims=[64, 32]):\n",
        "        super(ZIDeepSurv, self).__init__()\n",
        "        layers = []\n",
        "        current_dim = input_dim\n",
        "        for h_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(current_dim, h_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(0.2)) # Added dropout\n",
        "            current_dim = h_dim\n",
        "\n",
        "        self.feature_extractor = nn.Sequential(*layers)\n",
        "\n",
        "        # Output layers for pi, q, and beta\n",
        "        self.pi_layer = nn.Linear(current_dim, 1)\n",
        "        self.q_layer = nn.Linear(current_dim, 1)\n",
        "        self.beta_layer = nn.Linear(current_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.feature_extractor(x)\n",
        "\n",
        "        pi_logit = self.pi_layer(features)\n",
        "        q_logit = self.q_layer(features)\n",
        "        beta_raw = self.beta_layer(features)\n",
        "\n",
        "        pi = torch.sigmoid(pi_logit).squeeze(-1)\n",
        "        q = torch.sigmoid(q_logit).squeeze(-1)\n",
        "        beta = F.softplus(beta_raw).squeeze(-1) + 1e-6\n",
        "\n",
        "        return pi, q, beta\n",
        "\n",
        "\n",
        "class NoGNN_ZIS(nn.Module):\n",
        "    \"\"\"\n",
        "    Zero-Inflated Survival model using region-level features (mean covariates)\n",
        "    instead of a GNN for spatial information.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, region_feat_dim, hidden_dims=[64, 32]):\n",
        "        super(NoGNN_ZIS, self).__init__()\n",
        "        # Combine individual features (X) and region features\n",
        "        combined_dim = input_dim + region_feat_dim\n",
        "        layers = []\n",
        "        current_dim = combined_dim\n",
        "        for h_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(current_dim, h_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(0.2)) # Added dropout\n",
        "            current_dim = h_dim\n",
        "\n",
        "        self.feature_extractor = nn.Sequential(*layers)\n",
        "\n",
        "        self.pi_layer = nn.Linear(current_dim, 1)\n",
        "        self.q_layer = nn.Linear(current_dim, 1)\n",
        "        self.beta_layer = nn.Linear(current_dim, 1)\n",
        "\n",
        "    def forward(self, x, region_features_aggregated):\n",
        "        # Concatenate individual features with aggregated region features\n",
        "        combined_features = torch.cat([x, region_features_aggregated], dim=-1)\n",
        "        features = self.feature_extractor(combined_features)\n",
        "\n",
        "        pi_logit = self.pi_layer(features)\n",
        "        q_logit = self.q_layer(features)\n",
        "        beta_raw = self.beta_layer(features)\n",
        "\n",
        "        pi = torch.sigmoid(pi_logit).squeeze(-1)\n",
        "        q = torch.sigmoid(q_logit).squeeze(-1)\n",
        "        beta = F.softplus(beta_raw).squeeze(-1) + 1e-6\n",
        "\n",
        "        return pi, q, beta\n",
        "\n",
        "\n",
        "class ZISSurvNet_GNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Zero-Inflated Survival Network using a GCN layer to incorporate spatial information.\n",
        "    Individual features (X) are combined with region-level features learned by GCN.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, num_regions, gcn_hidden_dim=16, hidden_dims=[64, 32]):\n",
        "        super(ZISSurvNet_GNN, self).__init__()\n",
        "        if not HAS_PYG:\n",
        "            raise ImportError(\"torch_geometric is required for ZISSurvNet_GNN\")\n",
        "\n",
        "        self.num_regions = num_regions\n",
        "        self.gcn_hidden_dim = gcn_hidden_dim\n",
        "\n",
        "        # Initial linear layer to project individual features before aggregation\n",
        "        self.individual_proj = nn.Linear(input_dim, gcn_hidden_dim)\n",
        "\n",
        "        # GCN layer operating on the graph of regions\n",
        "        # Input to GCN will be aggregated features per region\n",
        "        self.gcn1 = GCNConv(gcn_hidden_dim, gcn_hidden_dim)\n",
        "        # self.gcn2 = GCNConv(gcn_hidden_dim, gcn_hidden_dim) # Optional second GCN layer\n",
        "\n",
        "        # Combine individual features (X) and GCN-processed region features\n",
        "        # Individual features (input_dim) + GCN output features (gcn_hidden_dim)\n",
        "        combined_dim = input_dim + gcn_hidden_dim\n",
        "        layers = []\n",
        "        current_dim = combined_dim\n",
        "        for h_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(current_dim, h_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(0.2)) # Added dropout\n",
        "            current_dim = h_dim\n",
        "\n",
        "        self.feature_extractor = nn.Sequential(*layers)\n",
        "\n",
        "        self.pi_layer = nn.Linear(current_dim, 1)\n",
        "        self.q_layer = nn.Linear(current_dim, 1)\n",
        "        self.beta_layer = nn.Linear(current_dim, 1)\n",
        "\n",
        "    def forward(self, x, region_ids, edge_index):\n",
        "        # Project individual features and aggregate to region level\n",
        "        individual_features_proj = self.individual_proj(x)\n",
        "\n",
        "        # Aggregate projected individual features by region (e.g., mean aggregation)\n",
        "        # Create a tensor to hold aggregated region features\n",
        "        region_features_agg = torch.zeros(self.num_regions, self.gcn_hidden_dim, device=x.device)\n",
        "\n",
        "        # Use torch_scatter or manual aggregation\n",
        "        # Manual aggregation:\n",
        "        for r in range(self.num_regions):\n",
        "            indices = (region_ids == r).nonzero(as_tuple=True)[0]\n",
        "            if indices.numel() > 0:\n",
        "                region_features_agg[r] = individual_features_proj[indices].mean(dim=0)\n",
        "\n",
        "\n",
        "        # Pass aggregated region features through GCN\n",
        "        # GCNConv requires edge_index and node features (region_features_agg)\n",
        "        # Optional: use edge_weight if available and meaningful\n",
        "        gcn_out = F.relu(self.gcn1(region_features_agg, edge_index))\n",
        "        # gcn_out = F.relu(self.gcn2(gcn_out, edge_index)) # Optional second GCN layer\n",
        "\n",
        "\n",
        "        # Map GCN output back to individuals based on their region_id\n",
        "        gcn_features_per_individual = gcn_out[region_ids]\n",
        "\n",
        "        # Concatenate original individual features with GCN features\n",
        "        combined_features = torch.cat([x, gcn_features_per_individual], dim=-1)\n",
        "\n",
        "        # Pass combined features through the final feature extractor\n",
        "        features = self.feature_extractor(combined_features)\n",
        "\n",
        "        pi_logit = self.pi_layer(features)\n",
        "        q_logit = self.q_layer(features)\n",
        "        beta_raw = self.beta_layer(features)\n",
        "\n",
        "        pi = torch.sigmoid(pi_logit).squeeze(-1)\n",
        "        q = torch.sigmoid(q_logit).squeeze(-1)\n",
        "        beta = F.softplus(beta_raw).squeeze(-1) + 1e-6\n",
        "\n",
        "        return pi, q, beta\n",
        "\n",
        "\n",
        "class ZISSimulation:\n",
        "    def __init__(self, num_simulations=100, epochs=100, lr=1e-3, device='cpu', out_folder=\"zis_sim_results\", h=1.0, epsilon=1e-5):\n",
        "        self.num_simulations = num_simulations\n",
        "        self.epochs = epochs\n",
        "        self.lr = lr\n",
        "        self.device = device\n",
        "        self.out_folder = out_folder\n",
        "        self.h = h\n",
        "        self.epsilon = epsilon\n",
        "        self.rx, self.ry = 10, 5\n",
        "        self.num_regions = self.rx * self.ry\n",
        "        self.region_coords = [(i,j) for i in range(self.rx) for j in range(self.ry)]\n",
        "        self.adj_matrix = self._build_lattice_adj()\n",
        "        self.scenarios = self._define_scenarios()\n",
        "        self.results_accumulator = {s: {\n",
        "            'DeepSurv': [],\n",
        "            'No-GNN-ZIS': [],\n",
        "            'ZI-DeepSurv': [],\n",
        "            'ZIS-SurvNet': []\n",
        "        } for s in self.scenarios}  # BICAR-ZIDW حذف شد\n",
        "        self.final_df = None\n",
        "        self.last_sim_data = None\n",
        "\n",
        "    def _build_lattice_adj(self):\n",
        "        G = nx.grid_2d_graph(self.rx, self.ry)\n",
        "        R = self.num_regions\n",
        "        adj = np.zeros((R, R), dtype=float)\n",
        "        for idx1, coord1 in enumerate(self.region_coords):\n",
        "            for idx2, coord2 in enumerate(self.region_coords):\n",
        "                if idx1 == idx2:\n",
        "                    continue\n",
        "                d = nx.shortest_path_length(G, coord1, coord2)\n",
        "                if d == 1:\n",
        "                    adj[idx1, idx2] = 1\n",
        "                else:\n",
        "                    adj[idx1, idx2] = np.exp(-d / self.h)\n",
        "        return adj\n",
        "\n",
        "    def _define_scenarios(self):\n",
        "        sigma1_moderate = np.array([[1., 0.612], [0.612, 1.5]])\n",
        "        sigma1_strong = np.array([[2., 1.959], [1.959, 3.]])\n",
        "        return {\n",
        "            1: {'desc': 'Moderate zero inflation, rho=0.5', 'Sigma': sigma1_moderate, 'GAMMA0': -1.5, 'ALPHA0': 0.5, 'BETA_SURV': 1.2, 'CENSOR_P': 0.1, 'clip_q': (1e-4, 0.99), 'clip_pi': (0.01, 0.99), 'max_time': 20},\n",
        "            2: {'desc': 'High zero inflation', 'Sigma': sigma1_moderate, 'GAMMA0': -0.4, 'ALPHA0': 0.5, 'BETA_SURV': 1.2, 'CENSOR_P': 0.1, 'clip_q': (1e-4, 0.99), 'clip_pi': (0.01, 0.99), 'max_time': 20},\n",
        "            3: {'desc': 'Strong spatial dependence', 'Sigma': sigma1_strong, 'GAMMA0': -1.5, 'ALPHA0': 0.5, 'BETA_SURV': 1.2, 'CENSOR_P': 0.1, 'clip_q': (1e-4, 0.99), 'clip_pi': (0.01, 0.99), 'max_time': 20}\n",
        "        }\n",
        "\n",
        "    def _generate_dataset(self, seed, **kwargs):\n",
        "        # بدون تغییر (مشابه کد اصلی)\n",
        "        np.random.seed(seed)\n",
        "        num_individuals = kwargs.get('num_individuals', 2000)\n",
        "        X_dim = 5\n",
        "        region_ids = np.random.choice(self.num_regions, num_individuals)\n",
        "        X = np.zeros((num_individuals, X_dim))\n",
        "        X[:, 0] = np.random.normal(0, 1, num_individuals)\n",
        "        X[:, 1] = np.random.uniform(0, 1.5, num_individuals)\n",
        "        X[:, 2] = np.random.binomial(1, 0.4, num_individuals)\n",
        "        X[:, 3] = np.random.poisson(3, num_individuals)\n",
        "        X[:, 4] = np.random.negative_binomial(2, 0.6, num_individuals)\n",
        "\n",
        "        R = self.num_regions\n",
        "        adj = self.adj_matrix\n",
        "        D = np.diag(adj.sum(axis=1))\n",
        "        Q = D - adj + self.epsilon * np.eye(R)\n",
        "        inv_Q = np.linalg.inv(Q)\n",
        "        Sigma = kwargs['Sigma']\n",
        "        cov_matrix = np.kron(inv_Q, Sigma)\n",
        "        phi_flat = np.random.multivariate_normal(mean=np.zeros(R*2), cov=cov_matrix)\n",
        "        phi = phi_flat.reshape((R, 2))\n",
        "\n",
        "        # Ensure ALPHA and GAMMA are provided in kwargs\n",
        "        ALPHA = kwargs.get('ALPHA', np.zeros(X_dim))\n",
        "        GAMMA = kwargs.get('GAMMA', np.zeros(X_dim))\n",
        "\n",
        "\n",
        "        lin_q = kwargs['ALPHA0'] + X.dot(ALPHA) + phi[region_ids, 1]\n",
        "        q = np.exp(-np.exp(lin_q))\n",
        "        q = np.clip(q, kwargs['clip_q'][0], kwargs['clip_q'][1])\n",
        "\n",
        "        lin_pi = kwargs['GAMMA0'] + X.dot(GAMMA) + phi[region_ids, 0]\n",
        "        pi = 1 / (1 + np.exp(-lin_pi))\n",
        "        pi = np.clip(pi, kwargs['clip_pi'][0], kwargs['clip_pi'][1])\n",
        "\n",
        "        T = np.zeros(num_individuals, dtype=int)\n",
        "        Y = np.zeros(num_individuals, dtype=int)\n",
        "        for i in range(num_individuals):\n",
        "            if np.random.rand() < pi[i]:\n",
        "                T[i] = 0\n",
        "                Y[i] = 0\n",
        "            else:\n",
        "                T_i = 1\n",
        "                while True:\n",
        "                    S_Ti = q[i] ** (T_i ** kwargs['BETA_SURV'])\n",
        "                    if np.random.rand() > S_Ti or T_i >= kwargs['max_time']:\n",
        "                        T[i] = T_i\n",
        "                        Y[i] = T_i\n",
        "                        break\n",
        "                    T_i += 1\n",
        "\n",
        "        C = np.random.geometric(kwargs['CENSOR_P'], num_individuals)\n",
        "        delta = (Y <= C).astype(int)\n",
        "        Y = np.minimum(Y, C)\n",
        "\n",
        "        df = pd.DataFrame(X, columns=[f'x{i+1}' for i in range(X_dim)])\n",
        "        df['region_id'] = region_ids\n",
        "        df['phi1'] = phi[region_ids, 0]\n",
        "        df['phi2'] = phi[region_ids, 1]\n",
        "        df['pi_true'] = pi\n",
        "        df['q_true'] = q\n",
        "        df['T'] = T\n",
        "        df['Y'] = Y\n",
        "        df['delta'] = delta\n",
        "        df['is_zero'] = (df['Y'] == 0).astype(int)\n",
        "\n",
        "        self.last_sim_data = df\n",
        "\n",
        "        summary = {\n",
        "            'prop_zero': df['is_zero'].mean(),\n",
        "            'prop_censored': (1 - df['delta']).mean()\n",
        "        }\n",
        "        return df, X, region_ids, summary\n",
        "\n",
        "    def _train_and_evaluate(self, df, X, region_ids, adj_matrix, **kwargs):\n",
        "        metrics = {}\n",
        "        X_t = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "        region_ids_t = torch.tensor(region_ids, dtype=torch.long).to(self.device)\n",
        "        Y_t = torch.tensor(df['Y'].values, dtype=torch.float32).to(self.device)\n",
        "        delta_t = torch.tensor(df['delta'].values, dtype=torch.float32).to(self.device)\n",
        "\n",
        "        edge_index = None\n",
        "        if HAS_PYG:\n",
        "            rows, cols = np.where(adj_matrix > 0)\n",
        "            edge_index = torch.tensor(np.vstack([rows, cols]), dtype=torch.long).to(self.device)\n",
        "            # edge_weight = torch.tensor(adj_matrix[rows, cols], dtype=torch.float32).to(self.device) # edge_weight not used in GCNConv here\n",
        "        # else: edge_weight = None # edge_weight not used\n",
        "\n",
        "        R = adj_matrix.shape[0]\n",
        "        region_features = torch.zeros(R, X.shape[1], dtype=torch.float32).to(self.device)\n",
        "        for r in range(R):\n",
        "            indices = (region_ids_t == r).nonzero(as_tuple=True)[0]\n",
        "            if indices.numel() > 0:\n",
        "                region_features[r] = X_t[indices].mean(dim=0)\n",
        "\n",
        "        models_to_run = {\n",
        "            'DeepSurv': DeepSurvLike(input_dim=X.shape[1]).to(self.device),\n",
        "            'No-GNN-ZIS': NoGNN_ZIS(input_dim=X.shape[1], region_feat_dim=X.shape[1]).to(self.device),\n",
        "            'ZI-DeepSurv': ZIDeepSurv(input_dim=X.shape[1]).to(self.device),\n",
        "            'ZIS-SurvNet': ZISSurvNet_GNN(input_dim=X.shape[1], num_regions=R).to(self.device)\n",
        "        }\n",
        "\n",
        "        for name, model in models_to_run.items():\n",
        "            try:\n",
        "                optimizer = Adam(model.parameters(), lr=self.lr)\n",
        "                for epoch in range(self.epochs): # Corrected loop variable name\n",
        "                    model.train()\n",
        "                    optimizer.zero_grad()\n",
        "                    if name == 'ZIS-SurvNet':\n",
        "                        # Check if edge_index is None and HAS_PYG is False\n",
        "                        if not HAS_PYG:\n",
        "                             print(f\"Skipping training for {name}: torch_geometric not available.\")\n",
        "                             continue\n",
        "                        pi, q, beta = model(X_t, region_ids_t, edge_index=edge_index)\n",
        "                        loss = zis_nll_loss(pi, q, beta, Y_t, delta_t)\n",
        "                    elif name == 'No-GNN-ZIS':\n",
        "                        pi, q, beta = model(X_t, region_features[region_ids_t])\n",
        "                        loss = zis_nll_loss(pi, q, beta, Y_t, delta_t)\n",
        "                    elif name == 'ZI-DeepSurv':\n",
        "                        pi, q, beta = model(X_t)\n",
        "                        loss = zis_nll_loss(pi, q, beta, Y_t, delta_t)\n",
        "                    else: # DeepSurv\n",
        "                        q, beta = model(X_t)\n",
        "                        loss = surv_nll_loss(q, beta, Y_t, delta_t)\n",
        "\n",
        "                    # Check for NaN loss\n",
        "                    if torch.isnan(loss):\n",
        "                         print(f\"NaN loss encountered for {name} in epoch {epoch}. Skipping.\")\n",
        "                         break # Exit epoch loop for this model\n",
        "\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    if name in ['ZIS-SurvNet', 'No-GNN-ZIS', 'ZI-DeepSurv']:\n",
        "                        if name == 'ZIS-SurvNet':\n",
        "                            if not HAS_PYG:\n",
        "                                 print(f\"Skipping evaluation for {name}: torch_geometric not available.\")\n",
        "                                 continue # Skip evaluation if GNN was skipped\n",
        "                            pi_pred_t, q_pred_t, beta_t = model(X_t, region_ids_t, edge_index=edge_index)\n",
        "                        elif name == 'No-GNN-ZIS':\n",
        "                            pi_pred_t, q_pred_t, beta_t = model(X_t, region_features[region_ids_t])\n",
        "                        else: # ZI-DeepSurv\n",
        "                            pi_pred_t, q_pred_t, beta_t = model(X_t)\n",
        "\n",
        "                        pi_pred = pi_pred_t.cpu().numpy()\n",
        "                        q_pred = q_pred_t.cpu().numpy()\n",
        "                        # Ensure beta_t is converted to a numpy array correctly\n",
        "                        beta_val = beta_t.cpu().numpy()\n",
        "\n",
        "\n",
        "                        # Metric calculations for ZIS models\n",
        "                        # Ensure beta_val is broadcastable if it's a single value\n",
        "                        if beta_val.ndim == 0: # Check if it's a scalar\n",
        "                             beta_val = np.full_like(q_pred, beta_val) # Broadcast scalar to shape of q_pred\n",
        "                        elif beta_val.shape != q_pred.shape:\n",
        "                             # If beta is per-individual but shapes don't match (shouldn't happen with squeeze(-1)),\n",
        "                             # you might need a different broadcasting strategy or an error\n",
        "                             print(f\"Warning: beta_val shape {beta_val.shape} does not match q_pred shape {q_pred.shape} for {name}.\")\n",
        "                             # Attempt to broadcast if possible, or handle error\n",
        "                             try:\n",
        "                                 beta_val = np.broadcast_to(beta_val, q_pred.shape)\n",
        "                             except ValueError:\n",
        "                                 print(f\"Error broadcasting beta_val for {name}. Skipping metrics.\")\n",
        "                                 metrics[name] = {}\n",
        "                                 continue\n",
        "\n",
        "\n",
        "                        p_zero = pi_pred + (1 - pi_pred) * (1 - q_pred ** beta_val) # P(Y=0)\n",
        "                        metrics[name] = {\n",
        "                            'NLL': zis_nll_loss(pi_pred_t, q_pred_t, beta_t, Y_t, delta_t).item(),\n",
        "                            'AUC': roc_auc_score(df['is_zero'], -p_zero), # AUC for predicting zero (event)\n",
        "                            'MAE_pi': mean_absolute_error(df['pi_true'], pi_pred),\n",
        "                            'Bias_pi': np.mean(pi_pred - df['pi_true']),\n",
        "                            'MSE_pi': mean_squared_error(df['pi_true'], pi_pred),\n",
        "                            'Brier_pi': mean_squared_error(df['is_zero'], p_zero), # Brier score for zero prediction\n",
        "                            'MSE_q': mean_squared_error(df['q_true'], q_pred),\n",
        "                            'Bias_q': np.mean(q_pred - df['q_true']),\n",
        "                            'Bias_beta': np.mean(beta_val - kwargs['BETA_SURV']) # Calculate mean bias for beta\n",
        "                        }\n",
        "                        # MSE_total is not standard, maybe remove or clarify\n",
        "                        # metrics[name]['MSE_total'] = metrics[name]['Brier_pi'] + metrics[name]['MSE_q']\n",
        "\n",
        "                    else: # DeepSurv\n",
        "                        q_pred_t, beta_t = model(X_t)\n",
        "                        q_pred = q_pred_t.cpu().numpy()\n",
        "                        # Ensure beta_t is converted to a numpy array correctly\n",
        "                        beta_val = beta_t.cpu().numpy()\n",
        "                        if beta_val.ndim == 0: # Check if it's a scalar\n",
        "                             beta_val = np.full_like(q_pred, beta_val) # Broadcast scalar to shape of q_pred\n",
        "                        elif beta_val.shape != q_pred.shape:\n",
        "                             print(f\"Warning: beta_val shape {beta_val.shape} does not match q_pred shape {q_pred.shape} for {name}.\")\n",
        "                             try:\n",
        "                                 beta_val = np.broadcast_to(beta_val, q_pred.shape)\n",
        "                             except ValueError:\n",
        "                                 print(f\"Error broadcasting beta_val for {name}. Skipping metrics.\")\n",
        "                                 metrics[name] = {}\n",
        "                                 continue\n",
        "\n",
        "\n",
        "                        p_zero = 1 - q_pred ** beta_val # DeepSurv predicts P(Y=0) implicitly\n",
        "\n",
        "                        metrics[name] = {\n",
        "                            'NLL': surv_nll_loss(q_pred_t, beta_t, Y_t, delta_t).item(),\n",
        "                            'AUC': roc_auc_score(df['is_zero'], -p_zero), # AUC for predicting zero (event)\n",
        "                            'Brier_pi': mean_squared_error(df['is_zero'], p_zero), # Brier score for zero prediction\n",
        "                            'MSE_q': mean_squared_error(df['q_true'], q_pred),\n",
        "                            'Bias_q': np.mean(q_pred - df['q_true']),\n",
        "                            'Bias_beta': np.mean(beta_val - kwargs['BETA_SURV']) # Calculate mean bias for beta\n",
        "                        }\n",
        "                        # MSE_total not standard\n",
        "                        # metrics[name]['MSE_total'] = metrics[name]['Brier_pi'] + metrics[name]['MSE_q']\n",
        "\n",
        "                    # Survival metrics (C-index, IBS)\n",
        "                    # For ZIS models, risk score could be related to expected time or P(Y=0)\n",
        "                    # A common approach is to use -log(S(y)) or similar from the non-zero part, or P(Y>0)\n",
        "                    # Let's use P(Y>0) = 1 - P(Y=0) as a risk score (higher P(Y>0) means lower risk of zero, higher survival potential)\n",
        "                    # Or use the non-zero survival function S_nz(t)\n",
        "                    # Let's use the risk score from the non-zero component: -log(-log(np.clip(q_pred, 1e-6, 1 - 1e-6)))\n",
        "                    # Use the negative expected time in the non-zero component as risk score\n",
        "                    # Calculate expected time for non-zero component\n",
        "                    max_sum_t_risk = kwargs['max_time'] + 20\n",
        "                    t_sum_risk = np.arange(1, max_sum_t_risk + 1)\n",
        "                    q_pred_broadcast_risk = q_pred[:, np.newaxis]\n",
        "                    beta_val_broadcast_risk = beta_val[:, np.newaxis]\n",
        "                    S_nz_sum_risk = q_pred_broadcast_risk ** (t_sum_risk[np.newaxis, :] ** beta_val_broadcast_risk)\n",
        "                    E_T_nz_risk = np.sum(S_nz_sum_risk, axis=1)\n",
        "\n",
        "                    risk_scores = -E_T_nz_risk # Use negative expected time as risk score\n",
        "\n",
        "\n",
        "                    cidx = lifelines_cindex(df['Y'].values, -risk_scores, df['delta'].astype(bool)) if HAS_LIFELINES else manual_c_index(df['Y'].values, df['delta'].values, risk_scores)\n",
        "                    metrics[name]['C-index'] = cidx\n",
        "\n",
        "                    # IBS needs predicted survival probabilities S(t)\n",
        "                    # For ZIS: S(t) = (1-pi) * S_nz(t) + pi * I(t=0)\n",
        "                    # Since we are evaluating at times t >= 1, S(t) = (1-pi) * S_nz(t) for t>=1\n",
        "                    times = np.arange(1, kwargs['max_time'] + 1) # Evaluate up to max_time\n",
        "                    if name in ['ZIS-SurvNet', 'No-GNN-ZIS', 'ZI-DeepSurv']:\n",
        "                        # S(t) = (1-pi) * q^t^beta for t > 0\n",
        "                         # Ensure pi_pred and beta_val are correctly shaped for broadcasting\n",
        "                        pi_pred_broadcast = pi_pred[:, np.newaxis]\n",
        "                        q_pred_broadcast = q_pred[:, np.newaxis]\n",
        "                        beta_val_broadcast = beta_val[:, np.newaxis] # Ensure beta is also broadcastable\n",
        "\n",
        "                        surv_probs = (1 - pi_pred_broadcast) * (q_pred_broadcast ** (times[np.newaxis, :] ** beta_val_broadcast))\n",
        "\n",
        "                    else: # DeepSurv\n",
        "                        # S(t) = q^t^beta for t > 0\n",
        "                        q_pred_broadcast = q_pred[:, np.newaxis]\n",
        "                        beta_val_broadcast = beta_val[:, np.newaxis]\n",
        "                        surv_probs = q_pred_broadcast ** (times[np.newaxis, :] ** beta_val_broadcast)\n",
        "\n",
        "\n",
        "                    # Ensure surv_probs is numpy array for lifelines_cindex and simple_ibs_discrete\n",
        "                    surv_probs_np = surv_probs # surv_probs should already be numpy from the calculations above\n",
        "\n",
        "                    ibs_val = integrated_brier_score(df['Y'].values, df['delta'].astype(bool), surv_probs_np, times) if HAS_LIFELINES else simple_ibs_discrete(df['Y'].values, df['delta'].values, q_pred, beta_val, times=times)[0]\n",
        "\n",
        "                    metrics[name]['IBS'] = ibs_val\n",
        "\n",
        "                    # MAE_exp calculation (Mean Absolute Error of Expected Non-Zero Time)\n",
        "                    # Expected time for non-zero component E[T_nz] = sum_{t=1}^inf S_nz(t)\n",
        "                    mask = (df['Y'] > 0) & (df['delta'] == 1) # Only consider observed non-zero event times for this metric\n",
        "                    if mask.sum() > 0:\n",
        "                        q_pred_m = q_pred[mask]\n",
        "                        beta_m = beta_val[mask] # Get beta values for the masked individuals\n",
        "\n",
        "                        # Estimate E[T_nz] by summing S_nz(t) up to a large time\n",
        "                        max_sum_t = kwargs['max_time'] + 20 # Sum beyond max_time to approximate infinity\n",
        "                        t_sum = np.arange(1, max_sum_t + 1) # Sum from t=1\n",
        "                        # S_nz(t) = q^t^beta\n",
        "                        # Ensure q_pred_m and beta_m are correctly shaped for broadcasting\n",
        "                        q_pred_m_broadcast = q_pred_m[:, np.newaxis]\n",
        "                        beta_m_broadcast = beta_m[:, np.newaxis]\n",
        "\n",
        "                        S_nz_sum = q_pred_m_broadcast ** (t_sum[np.newaxis, :] ** beta_m_broadcast)\n",
        "                        E_T_nz = np.sum(S_nz_sum, axis=1)\n",
        "\n",
        "                        # The MAE_exp should compare observed non-zero event times (df['Y'][mask])\n",
        "                        # with the expected time from the non-zero component (E_T_nz).\n",
        "                        # This metric assesses how well the model predicts the magnitude of non-zero times.\n",
        "                        mae_exp = mean_absolute_error(df['Y'][mask], E_T_nz)\n",
        "                        metrics[name]['MAE_exp'] = mae_exp\n",
        "                    else:\n",
        "                        metrics[name]['MAE_exp'] = np.nan\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error training or evaluating {name}: {e}\")\n",
        "                metrics[name] = {}\n",
        "\n",
        "        return metrics\n",
        "\n",
        "\n",
        "    def run_full_pipeline(self):\n",
        "        os.makedirs(self.out_folder, exist_ok=True)\n",
        "        for s, scenario_params in self.scenarios.items():\n",
        "            print(f\"\\n=== Running scenario {s}: {scenario_params['desc']} ===\")\n",
        "            for model_name in self.results_accumulator[s].keys():\n",
        "                self.results_accumulator[s][model_name] = []\n",
        "\n",
        "            for sim in tqdm(range(self.num_simulations), desc=f\"Scenario {s} sims\"):\n",
        "                seed = sim + s * 1000\n",
        "                df, X, region_ids, summary = self._generate_dataset(\n",
        "                    seed=seed,\n",
        "                    ALPHA=np.array([0.3, 0.2, -0.4, 0.1, 0.3]),\n",
        "                    GAMMA=np.array([0.5, -0.3, 0.4, 0.2, -0.1]),\n",
        "                    **scenario_params\n",
        "                )\n",
        "                metrics = self._train_and_evaluate(df, X, region_ids, self.adj_matrix, **scenario_params)\n",
        "                for m in self.results_accumulator[s]:\n",
        "                    if metrics.get(m):\n",
        "                        self.results_accumulator[s][m].append(metrics[m])\n",
        "\n",
        "        self._aggregate_results()\n",
        "        self._plot_all_figures()\n",
        "\n",
        "    def _aggregate_results(self):\n",
        "        rows = []\n",
        "        for s, models in self.results_accumulator.items():\n",
        "            for m, metric_list in models.items():\n",
        "                if not metric_list:\n",
        "                    continue\n",
        "                metric_names = metric_list[0].keys()\n",
        "                agg = {'Scenario': f\"Scenario {s}\", 'Model': m}\n",
        "                for metric in metric_names:\n",
        "                    vals = np.array([res[metric] for res in metric_list if metric in res and not np.isnan(res[metric])])\n",
        "                    if len(vals) > 1:\n",
        "                        mean = np.mean(vals)\n",
        "                        std = np.std(vals)\n",
        "                        df = len(vals) - 1\n",
        "                        # Handle case with very few data points for t-distribution\n",
        "                        if df > 0:\n",
        "                           t_val = stats.t.ppf(0.975, df)\n",
        "                           se = std / np.sqrt(len(vals))\n",
        "                           ci_low = mean - t_val * se\n",
        "                           ci_high = mean + t_val * se\n",
        "                           agg[metric] = f\"{mean:.3f} ({ci_low:.3f}–{ci_high:.3f})\"\n",
        "                        else:\n",
        "                           agg[metric] = f\"{mean:.3f} (N/A)\" # Not enough data for CI\n",
        "                    elif len(vals) == 1:\n",
        "                         agg[metric] = f\"{vals[0]:.3f} (N/A)\" # Only one data point\n",
        "                    else:\n",
        "                        agg[metric] = \"N/A (N/A)\" # No data points\n",
        "                rows.append(agg)\n",
        "\n",
        "        self.final_df = pd.DataFrame(rows)\n",
        "        self.final_df.to_csv(os.path.join(self.out_folder, \"aggregated_results.csv\"), index=False)\n",
        "        with open(os.path.join(self.out_folder, \"raw_results.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(self.results_accumulator, f)\n",
        "        print(\"\\n=== Final Aggregated Results ===\")\n",
        "        print(self.final_df.to_string(index=False))\n",
        "\n",
        "    def _plot_all_figures(self):\n",
        "        if self.last_sim_data is not None:\n",
        "            df = self.last_sim_data\n",
        "\n",
        "            # 1. Histograms of Covariates\n",
        "            df[['x1','x2','x3','x4','x5']].hist(bins=30, figsize=(12,8))\n",
        "            plt.suptitle(\"Histograms of Covariates\", fontsize=16)\n",
        "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "            plt.show()\n",
        "            plt.close()\n",
        "\n",
        "            # 2. Distribution of Survival and Observed Time\n",
        "            plt.figure(figsize=(10,6))\n",
        "            plt.hist(df['T'], bins=40, alpha=0.6, label='True Survival Time (T)')\n",
        "            plt.hist(df['Y'], bins=40, alpha=0.6, label='Observed Time (Y)')\n",
        "            plt.xlabel(\"Time\")\n",
        "            plt.ylabel(\"Count\")\n",
        "            plt.legend()\n",
        "            plt.title(\"Distribution of True and Observed Times\")\n",
        "            plt.show()\n",
        "            plt.close()\n",
        "\n",
        "            # 3. Kaplan-Meier Survival Curve\n",
        "            if HAS_LIFELINES:\n",
        "                kmf = KaplanMeierFitter()\n",
        "                kmf.fit(df['Y'], event_observed=df['delta'])\n",
        "                kmf.plot_survival_function()\n",
        "                plt.title(\"Kaplan-Meier Survival Curve (Observed Data)\")\n",
        "                plt.show()\n",
        "                plt.close()\n",
        "            else:\n",
        "                print(\"Skipping Kaplan-Meier plot: lifelines not available.\")\n",
        "\n",
        "            # 4. Censoring and Zero-Inflation Proportion\n",
        "            fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "            df['delta'].value_counts().plot.pie(ax=ax[0], autopct='%1.1f%%', labels=['Censored','Event'])\n",
        "            ax[0].set_title(\"Censoring Proportion\")\n",
        "            ax[0].set_ylabel('')\n",
        "            zero_counts = df['is_zero'].value_counts().sort_index()\n",
        "            if 0 not in zero_counts: zero_counts[0] = 0\n",
        "            if 1 not in zero_counts: zero_counts[1] = 0\n",
        "            zero_counts = zero_counts.sort_index()\n",
        "            ax[1].pie(zero_counts.values, labels=['Non-Zero Event','Zero Event'], autopct='%1.1f%%')\n",
        "            ax[1].set_title(\"Zero-Inflation Proportion\")\n",
        "            ax[1].set_ylabel('')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            plt.close()\n",
        "\n",
        "            # 5. Spatial Distribution Maps\n",
        "            if HAS_GEOPANDAS:\n",
        "                polygons = []\n",
        "                for x, y in self.region_coords:\n",
        "                    # Create polygon centered at (x, y) with side length 1\n",
        "                    poly = Polygon([(x-0.5, y-0.5), (x+0.5, y-0.5), (x+0.5, y+0.5), (x-0.5, y+0.5)])\n",
        "                    polygons.append(poly)\n",
        "\n",
        "                # Calculate mean pi_true and mean non-zero Y by region\n",
        "                pi_mean_by_r = df.groupby(\"region_id\")[\"pi_true\"].mean()\n",
        "                # Calculate mean of Y for non-zero values, reindex to include all regions and fill NaN with 0\n",
        "                Y_nonzero_mean_by_r = df[df[\"Y\"] > 0].groupby(\"region_id\")[\"Y\"].mean().reindex(np.arange(self.num_regions)).fillna(0)\n",
        "\n",
        "                gdf = gpd.GeoDataFrame({\"region_id\": np.arange(self.num_regions),\n",
        "                                        \"pi_mean\": pi_mean_by_r.values,\n",
        "                                        \"Y_nonzero_mean\": Y_nonzero_mean_by_r.values\n",
        "                                       }, geometry=polygons, crs=\"EPSG:4326\")\n",
        "\n",
        "                fig, axes = plt.subplots(1, 2, figsize=(16,6))\n",
        "\n",
        "                # Plot mean pi_true\n",
        "                gdf.plot(column=\"pi_mean\", ax=axes[0], cmap=\"coolwarm\", legend=True, edgecolor=\"black\", vmin=0, vmax=1)\n",
        "                axes[0].set_title(\"Spatial Distribution of True Mean pi (0–1)\", fontsize=14)\n",
        "                axes[0].set_xticks([])\n",
        "                axes[0].set_yticks([])\n",
        "\n",
        "                # Plot mean non-zero Y\n",
        "                # Use a colormap that handles 0 appropriately if needed, coolwarm is usually fine\n",
        "                gdf.plot(column=\"Y_nonzero_mean\", ax=axes[1], cmap=\"coolwarm\", legend=True, edgecolor=\"black\")\n",
        "                axes[1].set_title(\"Spatial Distribution of Mean Non-Zero Observed Y\", fontsize=14)\n",
        "                axes[1].set_xticks([])\n",
        "                axes[1].set_yticks([])\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "                plt.close()\n",
        "            else:\n",
        "                print(\"Skipping spatial maps: geopandas not available.\")\n",
        "\n",
        "        self._plot_results_boxplot()\n",
        "\n",
        "    def _plot_results_boxplot(self):\n",
        "        plot_rows_cidx = []\n",
        "        plot_rows_ibs = []\n",
        "        for s, models in self.results_accumulator.items():\n",
        "            for m, lst in models.items():\n",
        "                for res in lst:\n",
        "                    cval = res.get('C-index', np.nan)\n",
        "                    ibs_val = res.get('IBS', np.nan)\n",
        "                    plot_rows_cidx.append({'Scenario': f\"Scenario {s}\", 'Model': m, 'Metric': 'C-index', 'Value': cval})\n",
        "                    plot_rows_ibs.append({'Scenario': f\"Scenario {s}\", 'Model': m, 'Metric': 'IBS', 'Value': ibs_val})\n",
        "\n",
        "        plot_df_cidx = pd.DataFrame(plot_rows_cidx).dropna()\n",
        "        plot_df_ibs = pd.DataFrame(plot_rows_ibs).dropna()\n",
        "\n",
        "        if not plot_df_cidx.empty:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            sns.boxplot(x='Scenario', y='Value', hue='Model', data=plot_df_cidx)\n",
        "            plt.title('C-index distribution by Model and Scenario')\n",
        "            plt.ylabel('C-index')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            plt.close()\n",
        "\n",
        "        if not plot_df_ibs.empty:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            sns.boxplot(x='Scenario', y='Value', hue='Model', data=plot_df_ibs)\n",
        "            plt.title('IBS distribution by Model and Scenario')\n",
        "            plt.ylabel('IBS')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            plt.close()\n",
        "\n",
        "# Helper functions and Model classes (بدون تغییر)\n",
        "# ... (مانند کد اصلی)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Using device: {device}\")\n",
        "    NUM_SIMS = 10\n",
        "    EPOCHS = 5\n",
        "    simulator = ZISSimulation(num_simulations=NUM_SIMS, epochs=EPOCHS, device=device)\n",
        "    simulator.run_full_pipeline()\n"
      ]
    }
  ]
}